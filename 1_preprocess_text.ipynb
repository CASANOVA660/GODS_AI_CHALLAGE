{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import contractions\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from joblib import Parallel, delayed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# nltk.download(['punkt', 'punk_tab' 'stopwords', 'wordnet'])\n",
    "# nltk.download('punkt_tab')\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "# Preprocessing function (must be standalone and picklable)\n",
    "def preprocess_text(text, nlp):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Normalization\n",
    "    text = str(text).lower()\n",
    "    text = contractions.fix(text)\n",
    "    text = re.sub(r'@\\w+|http\\S+', '', text)\n",
    "    \n",
    "    # Cleaning (preserve punctuation)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Tokenization and filtering\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words and len(token) > 1]\n",
    "    \n",
    "    # Lemmatization with POS tagging\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    tokens = [token.lemma_ for token in doc if token.lemma_ != '-PRON-']\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [MISSING] Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in train data: 242\n",
      "Missing values in test data: 25\n",
      "Missing values in train data after [MISSING] imputation: 0\n",
      "Missing values in test data after [MISSING] imputation: 0\n"
     ]
    }
   ],
   "source": [
    "# Load your datasets\n",
    "train_df = pd.read_csv('data/train.csv')  # Replace with your actual train file\n",
    "test_df = pd.read_csv('data/test.csv')    # Replace with your actual test file\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"Missing values in train data: {train_df['content'].isna().sum()}\")\n",
    "print(f\"Missing values in test data: {test_df['content'].isna().sum()}\")\n",
    "\n",
    "train_df['content'] = train_df['content'].fillna('[MISSING]')\n",
    "test_df['content'] = test_df['content'].fillna('[MISSING]')\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"Missing values in train data after [MISSING] imputation: {train_df['content'].isna().sum()}\")\n",
    "print(f\"Missing values in test data after [MISSING] imputation: {test_df['content'].isna().sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper function for parallel processing\n",
    "def preprocess_batch(texts, nlp):\n",
    "    return Parallel(n_jobs=-1)(delayed(preprocess_text)(text, nlp) for text in texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "BrokenProcessPool",
     "evalue": "A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/awxlong/anaconda3/envs/gamma/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py\", line 426, in _process_worker\n    call_item = call_queue.get(block=True, timeout=timeout)\n  File \"/Users/awxlong/anaconda3/envs/gamma/lib/python3.9/multiprocessing/queues.py\", line 122, in get\n    return _ForkingPickler.loads(res)\nAttributeError: 'WordListCorpusReader' object has no attribute '_unload'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Preprocess text columns in parallel\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m preprocess_batch(test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m], nlp)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Save preprocessed data\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[30], line 3\u001b[0m, in \u001b[0;36mpreprocess_batch\u001b[0;34m(texts, nlp)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_batch\u001b[39m(texts, nlp):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_text\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gamma/lib/python3.9/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gamma/lib/python3.9/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/gamma/lib/python3.9/site-packages/joblib/parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[1;32m   1748\u001b[0m \n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[0;32m-> 1754\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/gamma/lib/python3.9/site-packages/joblib/parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1789\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gamma/lib/python3.9/site-packages/joblib/parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/gamma/lib/python3.9/site-packages/joblib/parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[0;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mBrokenProcessPool\u001b[0m: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable."
     ]
    }
   ],
   "source": [
    "# Preprocess text columns in parallel\n",
    "train_df['cleaned_text'] = preprocess_batch(train_df['content'], nlp)\n",
    "test_df['cleaned_text'] = preprocess_batch(test_df['content'], nlp)\n",
    "\n",
    "# Save preprocessed data\n",
    "train_df[['id', 'cleaned_text', 'target']].to_csv('data/train_preprocessed.csv', index=False)\n",
    "test_df[['id', 'cleaned_text', 'target']].to_csv('data/test_preprocessed.csv', index=False)\n",
    "\n",
    "print(\"Preprocessing complete! Files saved as train_preprocessed.csv and test_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "relationship-and-family-issues     6688\n",
       "anxiety                            6652\n",
       "depression                         5836\n",
       "ptsd-and-trauma                    1819\n",
       "suicidal-thoughts-and-self-harm    1156\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_df['content'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello cas fair anxiety depression work lot come work become much hard last year pretty good job waitress could cope high expectation overthought everything big give feedback know well end leave shift time wrong suppose start 11 thought 1 . freak badly answer call never go back time smoke plant cope really help give coming year work drive really anyone help 27 driving lesson expensive right want give work ago feel concerned long even get hire anywhere especially licence vicious cycle need licence car get job need job get licence car haha especially age feel optimistic able get back work gap resume centrelink work though many bill want get bit ahead life know go process manufacturing course july get forklift licence soon many job available hope would something easy would find something mostly worried manage anxiety work manage overthinke managing tell thing wrong concerned make mistake point get fire anyone something similar manage ok anyone able completely transparent workplace go type workplace hope find thank cas hi cas welcome detect underlie problem need tackle achieve download reliable worker thing like pay bill get drive lesson buy car fall place employer interested excuse people special need like we want worker problem free productive value pay wage manage reason forum popular people mind struggle place congregate share hide pain cafe employ physically disabled person disability would apparent flexibility would great experience 90 job 15 profession well separate work home life social life treatment apply job fork lift driver leave gap resume ask go overseas break work resume work job get job focus become reliable dealing issue mental illness outside work mean fill time sport hobby friend essentially expect much tolerance employer two part time job prefer fact distract well hope work google beyondblue topic never ever give beyondblue topic 30 minute save life tonywk hi cake feel empathise anxiety depression affect work work also mental health issue specific experience extent understand ... feel'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(train_df['content'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training data...\n"
     ]
    }
   ],
   "source": [
    "# Precompute stopwords as a global set\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# Load spaCy in a thread-safe way\n",
    "class SpacyProcessor:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "        \n",
    "    def process(self, text):\n",
    "        return self.nlp(text)\n",
    "\n",
    "# Initialize one instance per worker\n",
    "spacy_processor = SpacyProcessor()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Normalization\n",
    "    text = str(text).lower()\n",
    "    text = contractions.fix(text)\n",
    "    text = re.sub(r'@\\w+|http\\S+', '', text)\n",
    "    \n",
    "    # Cleaning\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Tokenization and filtering\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in STOPWORDS and len(t) > 1]\n",
    "    \n",
    "    # Lemmatization\n",
    "    doc = spacy_processor.process(\" \".join(tokens))\n",
    "    tokens = [token.lemma_ for token in doc if token.lemma_ != '-PRON-']\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "# Handle missing values\n",
    "train_df['content'] = train_df['content'].fillna('[MISSING]')\n",
    "test_df['content'] = test_df['content'].fillna('[MISSING]')\n",
    "\n",
    "# Parallel processing with thread-based backend\n",
    "def parallel_preprocess(df):\n",
    "    return Parallel(n_jobs=-1, backend='threading')(\n",
    "        delayed(preprocess_text)(text) for text in df['content']\n",
    "    )\n",
    "\n",
    "print(\"Preprocessing training data...\")\n",
    "train_df['cleaned_text'] = parallel_preprocess(train_df)\n",
    "\n",
    "print(\"Preprocessing test data...\")\n",
    "test_df['cleaned_text'] = parallel_preprocess(test_df)\n",
    "\n",
    "# Save results\n",
    "train_df[['id', 'cleaned_text', 'target']].to_csv('data/train_preprocessed.csv', index=False)\n",
    "test_df[['id', 'cleaned_text', 'target']].to_csv('data/test_preprocessed.csv', index=False)\n",
    "\n",
    "print(\"Preprocessing complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
